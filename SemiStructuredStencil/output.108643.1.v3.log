/capstor/scratch/cscs/ybudanaz/dace/dace/transformation/passes/simplify.py:117: UserWarning: LiftStructViews is not being applied due to incompatibility with experimental control flow blocks. If the SDFG does not contain experimental blocks, ensure the top level SDFG does not have `SDFG.using_explicit_control_flow` set to True. If LiftStructViews is compatible with experimental blocks, please annotate it with the class decorator `@dace.transformation.explicit_cf_compatible`. see `https://github.com/spcl/dace/wiki/Experimental-Control-Flow-Blocks` for more information.
  warnings.warn(p.__class__.__name__ + ' is not being applied due to incompatibility with ' +
Cannot inline SDFG kernel_21_8: 52 nested SDFG must have exactly one state. init <class 'dace.sdfg.state.SDFGState'>
Cannot inline SDFG kernel_21_8: 2 nested SDFG must have exactly one state. BinOp_23_2 <class 'dace.sdfg.state.SDFGState'>
Cannot inline SDFG kernel_21_8: 2 nested SDFG must have exactly one state. BinOp_23_2 <class 'dace.sdfg.state.SDFGState'>
Cannot inline SDFG kernel_21_8: 2 nested SDFG must have exactly one state. BinOp_23_2 <class 'dace.sdfg.state.SDFGState'>
Cannot inline SDFG kernel_21_8: 2 nested SDFG must have exactly one state. BinOp_23_2 <class 'dace.sdfg.state.SDFGState'>
Cannot inline SDFG kernel_21_8: 2 nested SDFG must have exactly one state. BinOp_23_2 <class 'dace.sdfg.state.SDFGState'>
Cannot inline SDFG kernel_21_8: 2 nested SDFG must have exactly one state. BinOp_23_2 <class 'dace.sdfg.state.SDFGState'>
Traceback (most recent call last):
  File "/capstor/scratch/cscs/ybudanaz/LayoutArtifacts/SemiStructuredStencil/unstructured_stencil_3d_u_s_u_v2_dace_v2.py", line 174, in <module>
    vals_A2, vals_B2, n2 = initialize(_N)
                           ^^^^^^^^^^^^^^
  File "/capstor/scratch/cscs/ybudanaz/LayoutArtifacts/SemiStructuredStencil/unstructured_stencil_3d_u_s_u_v2_dace_v2.py", line 72, in initialize
    vals_A = i * k * (j + 2) / N
             ~~~~~~^~~~~~~~~
torch.OutOfMemoryError: HIP out of memory. Tried to allocate 15.70 GiB. GPU 0 has a total capacity of 94.07 GiB of which 15.36 GiB is free. Of the allocated memory 62.89 GiB is allocated by PyTorch, and 15.60 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
